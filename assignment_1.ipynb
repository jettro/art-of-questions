{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Working with LLMs\n",
    "This assignment is all about interacting with LLMs. You can try HuggingFace or OpenAI. Running this part from your local machine is a bit harder. The models are called Large language Models for a reason.\n",
    "\n",
    "From now on, you will use Langchain to talk to large language models. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b019130f9d0fdbc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a1d0164353cb426"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HuggingFace LLM\n",
    "You can find more information about the HuggingFaceHub integration for Langchain [here](https://python.langchain.com/docs/integrations/llms/huggingface_hub#examples). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98e2ae463aba7932"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain, HuggingFaceHub\n",
    "\n",
    "\n",
    "def initialise_huggingface_llm(prompt: PromptTemplate, temperature: float = 0.1, max_length: int = 100) -> LLMChain:\n",
    "    \"\"\"\n",
    "    Creates the LangChain chain wrapper around the HuggingFaceHub interface.\n",
    "    \n",
    "    Parameters:\n",
    "        prompt (PromptTemplate): The prompt to pass to the llm\n",
    "        temperature (float): A higher temperature means the llm can have more fantasy, beware of hallucination\n",
    "        max_length (int): The maximum length of the text to generate in characters\n",
    "        \n",
    "    Returns:\n",
    "        LLMChain: The generic chain wrapper around an LLM.\n",
    "    \"\"\"\n",
    "    repo_id = \"google/flan-t5-xxl\"\n",
    "\n",
    "    # TODO initialise the llm using HuggingFaceHub\n",
    "    llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\": temperature, \"max_length\": max_length})\n",
    "\n",
    "    return LLMChain(prompt=prompt, llm=llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## OpenAI LLM\n",
    "You can find more information about the OpenAI integration for langchain [here](https://python.langchain.com/docs/integrations/llms/openai)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb23edeb5042edc3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "\n",
    "\n",
    "def initialise_openai_llm(prompt: PromptTemplate, temperature: float, max_length: int) -> LLMChain:\n",
    "    \"\"\"\n",
    "    Creates the LangChain chain wrapper around the OpenAI interface.\n",
    "    \n",
    "    Parameters:\n",
    "        prompt (PromptTemplate): The prompt to pass to the llm\n",
    "        temperature (float): A higher temperature means the llm can have more fantasy, beware of hallucination\n",
    "        max_length (int): The maximum length of the text to generate in characters\n",
    "\n",
    "    Returns:\n",
    "        LLMChain: The generic chain wrapper around an LLM.\n",
    "    \"\"\"\n",
    "    openai_model = \"text-davinci-003\"\n",
    "    \n",
    "    # TODO initialise the llm using OpenAI\n",
    "    llm = OpenAI(model_name=openai_model, temperature=temperature, max_tokens=max_length)\n",
    "    \n",
    "    return LLMChain(prompt=prompt, llm=llm)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "582a67dd46377307"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the prompt for the LLMs\n",
    "First you have to write the prompt to obtain information about a conference. The function _generate_prompt_ contains the code to generate the prompt. You only have to improve the prompt to helps us with our question."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71f4cb5bf0d71de9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "def generate_prompt() -> PromptTemplate:\n",
    "    \"\"\"\n",
    "    Generates a PromptTemplate containing one parameter for the conference\n",
    "    \n",
    "    Returns:\n",
    "        PromptTemplate: Template with one parameter representing the conference.\n",
    "    \"\"\"\n",
    "    # TODO: write the prompt to give information about a conference that is provided later on by the chain.\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"conference\"],\n",
    "        template=\"Generate a summary of what the conference {conference} is all about\"\n",
    "    )\n",
    "    \n",
    "    return prompt_template"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b918795e04f0588e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run the chain\n",
    "Now you can initialise the chains for both HuggingFace and OpenAI. After the initialisation, you can run the chains using the name of a conference."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66695f15c6f947d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: pass the right parameters to the chains\n",
    "conference_prompt = generate_prompt()\n",
    "conference_temperature = 0.1\n",
    "conference_max_length = 300\n",
    "\n",
    "hf_chain = initialise_huggingface_llm(prompt=conference_prompt, \n",
    "                                      temperature=conference_temperature,\n",
    "                                      max_length=conference_max_length)\n",
    "openai_chain = initialise_openai_llm(prompt=conference_prompt, \n",
    "                                     temperature=conference_temperature, \n",
    "                                     max_length=conference_max_length)\n",
    "\n",
    "conference_name = \"Devoxx\"\n",
    "\n",
    "print(\"HuggingFace\")\n",
    "print(hf_chain.run(conference_name))\n",
    "\n",
    "print(\"\\nOpenAI\")\n",
    "print(openai_chain.run(conference_name))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b853c5d1c9bd9ad0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Try changing the temperature and max_length parameters and analyse the results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21b661d2bd5e6be4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Write a prompt that returns a list of cities where the conference is held\n",
    "\"\"\"\n",
    "Generate a list of cities where the conference {conference} takes place. Stick to what you know of these locations.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f4da95e2d5fb091"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Write a prompt that returns a JSON document with a list of objects. Each object must contain the name of the country and the city.\n",
    "\"\"\"\n",
    "Generate a list of cities where the conference {conference} takes place. Stick to what you know of these locations. Return the list as a json object using the field names country and city\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cd798f0b1d4563a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
