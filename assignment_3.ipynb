{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Use Langchain to implement RAG\n",
    "First you implement the most basic RAG system possible using Langchain. When the defaults work, you make some adjustments to better control the output of the RAG system. \n",
    "\n",
    "## The basic RAG using defaults\n",
    "You need an LLM and a Retriever. You can choose OpenAI and Weaviate or alternatives. Use the Langchain wrappers to initialise them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee2ad17b43ec11a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# TODO: Implement the retriever through the vector store.\n",
    "\n",
    "import weaviate\n",
    "from langchain.vectorstores import Weaviate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "weaviate_url = os.getenv('WEAVIATE_CLUSTER_URL')\n",
    "\n",
    "auth_config = weaviate.auth.AuthApiKey(\n",
    "    api_key=os.getenv('WEAVIATE_API_KEY'),\n",
    ")\n",
    "\n",
    "weaviate_client = weaviate.Client(\n",
    "    url=weaviate_url,\n",
    "    auth_client_secret=auth_config,\n",
    "    additional_headers={\n",
    "        \"X-OpenAI-Api-Key\": os.getenv('OPEN_AI_API_KEY')\n",
    "    }\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=os.getenv('OPEN_AI_API_KEY'), model_name=\"gpt-4\")\n",
    "\n",
    "openai_embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=os.getenv('OPEN_AI_API_KEY'),\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "vector_store = Weaviate(client=weaviate_client,\n",
    "                        embedding=openai_embeddings,\n",
    "                        index_name=\"DevoxxFAQ\",\n",
    "                        text_key=\"text\",\n",
    "                        by_text=False)\n",
    "             "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf83616c6f115e91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Implement the chain using Chroma or Weaviate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "question_answer_chain = RetrievalQA.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e46744ca3fb8066a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query=\"How many proposals are received, and how many proposals can be accepted?\"\n",
    "print(question_answer_chain.run(query))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a174adf852b12924"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Experiment with more questions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d40911bb4ee1f37"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Replacing the defaults\n",
    "You have used most of the defaults. Still you can configure the different components using the same structure."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6df45661f88e8f36"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This one can be a bit tricky. You need to check the documents that were used to generate the answer. There is an option to also return the source documents. On which component would you add that option? Make the change so that the following block runs without errors."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10623c00c8847bb6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO change the previous code blocks in such a way that the next code blocks runs without errors and shows the result as well as the source documents.\n",
    "\n",
    "query=\"How many proposals are received, and how many proposals can be accepted?\"\n",
    "response = question_answer_chain({\"query\": query})\n",
    "\n",
    "print(response[\"result\"])\n",
    "print(\"--------------\")\n",
    "print(\"The source documents\")\n",
    "for doc in response[\"source_documents\"]: \n",
    "    print(doc.page_content)\n",
    "    print(\"-----\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5ab1af4b6a17e14"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next you want to be friendly to non-native english speakers and make it possible to return the answers in another language. Change the prompt in such a way that the result will be printed in Dutch."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6758a5a664e0d5b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO Adjust the prompt in such a way that the result is in another language.\n",
    "\n",
    "from langchain import PromptTemplate, OpenAI\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "    You are a system that writes an answer to the provided question using the provided context. Answer in Dutch.\n",
    "    \n",
    "    {context}\n",
    "\n",
    "    Question: {question}:\"\"\"\n",
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "question_answer_chain = RetrievalQA.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    prompt=custom_prompt,\n",
    "    return_source_documents=True\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query = \"How many proposals are received, and how many proposals can be accepted?\"\n",
    "response = question_answer_chain({\"query\": query})\n",
    "\n",
    "print(response[\"result\"])\n",
    "print(\"--------------\")\n",
    "print(\"The source documents\")\n",
    "for doc in response[\"source_documents\"]:\n",
    "    print(doc.page_content)\n",
    "    print(\"-----\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "466281044a4039e4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
