{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Using Unstructured to parse documents\n",
    "The [unstructured library](https://unstructured.io) provides open-source components for ingesting and pre-processing images and text documents, such as PDFs, HTML, Word docs, and many more.\n",
    "\n",
    "There are two ways to use unstructured:\n",
    "- Install required components yourself\n",
    "- Use the Unstructured API\n",
    "\n",
    "Registration is free, you can obtain a key here: [Obtain Unstructured Key](https://unstructured.io/#get-api-key). If you want to install it on you local machine, instructions for MacOS are in the [Langchain example playbook](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file). For Windows look at the [Unstructured documentation](https://unstructured-io.github.io/unstructured/installation/full_installation.html). We recommend using the remote API for the workshop.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c18ba8f99fb171e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The components\n",
    "You start your journey by looking at the different components setting up the chain. After that you combine all the components into a complete chain.\n",
    "\n",
    "The components you have to use are:\n",
    "- Document loaders: Multiple loaders are available. Later you use the csv loader, but you start with a loader for the Unstructured library. Using this loader we can extract content from a pdf.\n",
    "- Document transformer: When loading the complete text from a document into vector store, the text can be to big or have little meaning. Working with chunks of text overcomes that problem. You use a splitter to create chunks of text.\n",
    "- Text embedding: Vector stores are great for semantic search. You create an embedding vector that can be stored in a vector store.\n",
    "- Vector stores: Accept documents with embeddings and provide a way to search over the available vectors for a similar result to the query. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "539a3c82bcec061d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Document loader: load pdf using Unstructured\n",
    "The code block shows the code to use the API version of Unstructured. If you forget to add the API key, it tries to call a local version. If you see a message about missing libraries, you might have missed adding the api key to the environment.\n",
    "\n",
    "If you want to use a local running version, remove the _api_key_ from the call, and replace UnstructuredAPIFileLoader with UnstructuredFileLoader. You can find [more information](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file) on the Langchain website."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ced6ee922c878ce8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from unstructured.cleaners.core import clean_extra_whitespace\n",
    "from langchain.document_loaders import UnstructuredAPIFileLoader\n",
    "\n",
    "unstructured_api_key = os.getenv('UNSTRUCTURED_API_KEY')\n",
    "# TODO: Initialise the loader, add a clean_extra_whitespace as a post processor. The faq.pdf is in the data_sources folder. \n",
    "loader = None\n",
    "\n",
    "docs = loader.load()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba9925125e6052cc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Document transformer: create chunks using a splitter\n",
    "The complete document is too large to match similar queries. Therefore, we create chunks of knowledge to be more specific. Too large documents cannot be passed to an LLM.\n",
    "- We suggest a chunk_size of 300, and a chunk_overlap of 100. \n",
    "- Print the number of chunks after splitting the text of the pdf\n",
    "\n",
    "Use the RecursiveCharacterTextSplitter to split the text in the document into chunks. \n",
    "\n",
    "Play around with the chunk_size and the chunk_overlap. Notice the changes to the output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fde2037f45d61e21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# TODO: Initialise the splitter\n",
    "text_splitter = None\n",
    "\n",
    "chunks = text_splitter.create_documents(texts=[docs[0].page_content], metadatas=[docs[0].metadata])\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(f\"--\\n{chunk.page_content}\\n--\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd274aed6a1d74ff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text embedding\n",
    "There are multiple ways to generate embeddings, some are free, some have to be paid. We prefer the OpenAI embedding. Not an expensive one, but when embedding a lot of content you still have to monitor the costs.  \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be9df7b037aaabdf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use OpenAI\n",
    "In this block you create the embeddings using the OpenAI endpoint. You choose if you want to embed all chunks or just one. You can find more information [here](https://python.langchain.com/docs/integrations/text_embedding/openai)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48d41fbe29885fea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "openai_embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=os.getenv('OPEN_AI_API_KEY'),\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "# TODO: Create the embeddings or vectors for the first chunk\n",
    "vectors_openai = None\n",
    "\n",
    "# TODO: Calculate the dimension of embedding with the OpenAI model\n",
    "print(f\"Dimension of the embedding for OpenAI '{None}'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "418892b03c180018"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use HuggingFace Hub\n",
    "In this block you create the embeddings using the HuggingFace Hub endpoint. You choose if you want to embed all chunks or just one. You can find more information [here](https://python.langchain.com/docs/integrations/text_embedding/huggingfacehub)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7ccf877f63513ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
    "\n",
    "repo_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "huggingface_embeddings_hub = HuggingFaceHubEmbeddings(\n",
    "    repo_id=repo_id,\n",
    "    task=\"feature-extraction\",\n",
    "    huggingfacehub_api_token=os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    ")\n",
    "\n",
    "# TODO: Create the embeddings or vectors for the first chunk\n",
    "vectors_hfh = None\n",
    "\n",
    "# TODO: Calculate the dimension of embedding with the HuggingFace Hub model\n",
    "print(f\"Dimension of the embedding for HuggingFace Hub '{None}'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bca42c2b580a6ce9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use HuggingFace sentence transformers locally\n",
    "Beware, the model will be downloaded to your local machine. Advantage, you do not have to obtain some API key."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "450315f385600a95"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "huggingface_embeddings = HuggingFaceEmbeddings(model_name=model)\n",
    "\n",
    "# TODO: Create the embeddings or vectors for the first chunk\n",
    "vectors_hf = None\n",
    "\n",
    "# TODO: Calculate the dimension of embedding with the HuggingFace model\n",
    "print(f\"Dimension of the embedding for HuggingFace '{None}'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4b4c7bf854e4366"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Vector Stores: keeping the vectors safe and searchable\n",
    "Langchain supports a lot of vector stores. We start with _Chroma_, you can easily run this without any installation. Vector stores like Weaviate or OpenSearch have more functionality and scalability. One cool feature of Weaviate is the Hybrid search."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66f823b7fb398235"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chroma Vectorstore\n",
    "If you want, you can change the embedder that is used. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30ab86084d22cf50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "chroma_vs = Chroma.from_documents(chunks, openai_embeddings)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f27267dfe1832d67"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Weaviate vectorstore\n",
    "You can find more information about the Lanchchain integration with Weaviate [here](https://python.langchain.com/docs/integrations/vectorstores/weaviate)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3d6af43e5c03ac5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from langchain.vectorstores import Weaviate\n",
    "\n",
    "weaviate_url = os.getenv('WEAVIATE_URL')\n",
    "\n",
    "auth_config = weaviate.auth.AuthApiKey(\n",
    "    api_key=os.getenv('WEAVIATE_API_KEY'),\n",
    ")\n",
    "\n",
    "weaviate_client = weaviate.Client(\n",
    "    url=weaviate_url,\n",
    "    auth_client_secret=auth_config,\n",
    "    additional_headers={\n",
    "        \"X-OpenAI-Api-Key\": os.getenv('OPEN_AI_API_KEY')\n",
    "    }\n",
    ")\n",
    "\n",
    "# TODO: Initialise the Weaviate vector store\n",
    "weaviate_vs = None"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83466adc1941de20"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implement similarity search for one or both vector stores\n",
    "Use one or both of the vector stores to find the most similar chunks. Notice that you get back a chunk of text. In the next assignment, you use these chunks to generate an answer using a RAG solution."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e2df6baa216e942"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query = \"Who should I contact to become a sponsor?\"\n",
    "\n",
    "# TODO: call the vector store and find the chunks that are most similar to the query above.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5d0c8c23dda6c0e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
